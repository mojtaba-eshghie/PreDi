{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sympy==1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(message):\n",
    "    print(f\"{message}\")\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from typing import Union\n",
    "from src.parser import ASTNode\n",
    "from src.config import debug_print\n",
    "\n",
    "class Simplifier:\n",
    "    def __init__(self):\n",
    "        self.symbols = {\n",
    "            'msg.sender': sp.Symbol('msg_sender'),\n",
    "            'msg.origin': sp.Symbol('msg_origin'),\n",
    "            '==': sp.Eq,\n",
    "            '!=': sp.Ne,\n",
    "            '>=': sp.Ge,\n",
    "            '<=': sp.Le,\n",
    "            '>': sp.Gt,\n",
    "            '<': sp.Lt,\n",
    "            '&&': sp.And,\n",
    "            '||': sp.Or,\n",
    "            '!': sp.Not\n",
    "        }\n",
    "\n",
    "    def simplify(self, ast: ASTNode) -> Union[str, ASTNode]:\n",
    "        debug_print(f\"Simplifying AST: {ast}\")\n",
    "        sympy_expr = self._to_sympy(ast)\n",
    "        debug_print(f\"Converted to sympy expression: {sympy_expr}\")\n",
    "        simplified_expr = sp.simplify(sympy_expr)\n",
    "        debug_print(f\"Simplified sympy expression: {simplified_expr}\")\n",
    "        simplified_ast = self._to_ast(simplified_expr)\n",
    "        debug_print(f\"Converted back to AST: {simplified_ast}\")\n",
    "        return simplified_ast\n",
    "\n",
    "    def _to_sympy(self, node: ASTNode):\n",
    "        if node.value in self.symbols and not node.children:\n",
    "            return self.symbols[node.value]\n",
    "        elif node.value in self.symbols:\n",
    "            if node.value in ('&&', '||'):\n",
    "                return self.symbols[node.value](*[self._to_sympy(child) for child in node.children])\n",
    "            elif node.value == '!':\n",
    "                return self.symbols[node.value](self._to_sympy(node.children[0]))\n",
    "            elif len(node.children) == 2:\n",
    "                return self.symbols[node.value](self._to_sympy(node.children[0]), self._to_sympy(node.children[1]))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid number of children for operator {node.value}\")\n",
    "        elif isinstance(node.value, (int, float)):\n",
    "            return sp.Number(node.value)\n",
    "        else:\n",
    "            # Preserve function calls and other identifiers as-is\n",
    "            if '(' in node.value and ')' in node.value:\n",
    "                func_name = node.value  # Ensure the function name is preserved entirely\n",
    "                args = node.children\n",
    "                return sp.Function(func_name)(*map(self._to_sympy, args))\n",
    "            else:\n",
    "                return sp.Symbol(node.value.replace('.', '_'))\n",
    "\n",
    "    def _to_ast(self, expr):\n",
    "        if isinstance(expr, sp.Equality):\n",
    "            return ASTNode('==', [self._to_ast(expr.lhs), self._to_ast(expr.rhs)])\n",
    "        elif isinstance(expr, sp.Rel):\n",
    "            op_map = {'>': '>', '<': '<', '>=': '>=', '<=': '<=', '!=': '!='}\n",
    "            return ASTNode(op_map[expr.rel_op], [self._to_ast(expr.lhs), self._to_ast(expr.rhs)])\n",
    "        elif isinstance(expr, sp.And):\n",
    "            return ASTNode('&&', [self._to_ast(arg) for arg in expr.args])\n",
    "        elif isinstance(expr, sp.Or):\n",
    "            return ASTNode('||', [self._to_ast(arg) for arg in expr.args])\n",
    "        elif isinstance(expr, sp.Not):\n",
    "            return ASTNode('!', [self._to_ast(expr.args[0])])\n",
    "        elif isinstance(expr, sp.Function):\n",
    "            func_name = str(expr.func)\n",
    "            return ASTNode(func_name, [self._to_ast(arg) for arg in expr.args])\n",
    "        else:\n",
    "            return ASTNode(str(expr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "simplifier = Simplifier()\n",
    "\n",
    "tokens1 = tokenizer.tokenize(predicate1)\n",
    "parser1 = Parser(tokens1)\n",
    "ast1 = parser1.parse()\n",
    "simplified_ast1 = simplifier.simplify(ast1)\n",
    "\n",
    "print(f\"predicate1: {predicate1}\")\n",
    "print(f\"AST1 is: {ast1}\")\n",
    "print(f\"Simplified AST1: {simplified_ast1}\")\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------')\n",
    "\n",
    "tokens2 = tokenizer.tokenize(predicate2)\n",
    "parser2 = Parser(tokens2)\n",
    "ast2 = parser2.parse()\n",
    "simplified_ast2 = simplifier.simplify(ast2)\n",
    "\n",
    "print(f\"predicate1: {predicate2}\")\n",
    "print(f\"AST2 is: {ast2}\")\n",
    "print(f\"Simplified AST1: {simplified_ast2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate1, predicate2 = \"(_tTotalpercentBuy)/divisorBuy>=(_tTotal/5000)\", \"(percentBuy_decimals)/divisorBuy>=(_tTotal/10000)\"\n",
    "comparator = Comparator()\n",
    "result = comparator.compare(predicate1, predicate2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"GGmorello/FLAMES_results\", \"100k\", token='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')\n",
    "\n",
    "#ds = load_dataset('GGmorello/FLAMES', 'infilled', split='train[:10000]', token='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', cache_dir='/Users/mojtabaeshghie/.cache/hf')#, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k = ds['train'].to_pandas()\n",
    "df_100k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) For predicates E_0[_garbageAddress]==0 ************* E_0[_garbageAddress]==0 ############## The predicates are equivalent.\n",
      "(1) For predicates xrmToken.balanceOf(_user)>=_amount ************* _amount>0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(2) For predicates numberOfTokens+buyerBalance<=pre_mint_limit ************* numberOfTokens<=pre_mint_limit ############## The predicates are not equivalent and neither is stronger.\n",
      "(3) For predicates MathHelper.sumNumbers(_milestonesFundings).add(_finalReward)<=getUintConfig(CONFIG_MAX_FUNDING_FOR_NON_DIGIX) ************* _milestonesFundings.length>0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(4) For predicates msg.sender==governance||msg.sender==controller||msg.sender==address(this) ************* msg.sender==controller||msg.sender==governance ############## The predicates are not equivalent and neither is stronger.\n",
      "(5) For predicates addresses[j]!=0x0 ************* addresses[j]!=0x0 ############## The predicates are equivalent.\n",
      "(6) For predicates address(bytesContract)!=address(0) ************* bytesContract!=address(0) ############## The predicates are not equivalent and neither is stronger.\n",
      "(7) For predicates _addresses.length<=200 ************* _addresses.length>0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(8) For predicates msg.sender==_mintRequest.to ************* requiredPrice==msg.value ############## The predicates are not equivalent and neither is stronger.\n",
      "(9) For predicates owner()==data.creators[0].account ************* msg.sender==owner() ############## The predicates are not equivalent and neither is stronger.\n",
      "(10) For predicates tokenIdByGenerationWaveAndSerial[uint8(_generation)][uint8(_wave)][_serial]==0 ************* tokenIdByGenerationWaveAndSerial[uint8(_generation)][uint8(_wave)][_serial]==0 ############## The predicates are equivalent.\n",
      "(11) For predicates msg.sender==address(previousRegistrar) ************* deed.owner()==address(this) ############## The predicates are not equivalent and neither is stronger.\n",
      "(12) For predicates nimblToken.allowance(owner,address(this))>=nimblAmount ************* nimblToken.allowance(owner,address(this))>=nimblAmount ############## The predicates are equivalent.\n",
      "(13) For predicates ethBalances[_msgSender()]<=9e18 ************* tokens<=remainingTokens ############## The predicates are not equivalent and neither is stronger.\n",
      "(14) For predicates burnSanityCheck(tokens) ************* balances[msg.sender]>=tokens ############## The predicates are not equivalent and neither is stronger.\n",
      "(15) For predicates addrOf[digitdomain].id==0 ************* bytes(digitdomain).length!=0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(16) For predicates mint_num[msg.sender]+num<=saleConfig.max_num ************* mint_num[msg.sender]+num<=1 ############## The predicates are not equivalent and neither is stronger.\n",
      "(17) For predicates array.length==desired.length ************* desired.length==array.length ############## The predicates are equivalent.\n",
      "(18) For predicates msg.sender==_burnContract ************* msg.sender==_burnContract ############## The predicates are equivalent.\n",
      "(19) For predicates _resourceYields.length==resourceTypes.length ************* _resourceYields.length==resourceTypes.length ############## The predicates are equivalent.\n",
      "(20) For predicates IERC20(token).transfer(msg.sender,DISCOVERER_AMOUNT) ************* IERC20(token).transfer(msg.sender,DISCOVERER_AMOUNT) ############## The predicates are equivalent.\n",
      "(21) For predicates totalSupply+amount<=400 ************* amount<=3 ############## The predicates are not equivalent and neither is stronger.\n",
      "(22) For predicates balanceOf[msg.sender]>=_value*_decimals ************* balanceOf[msg.sender]>=_value*_decimals ############## The predicates are equivalent.\n",
      "(23) For predicates totalSupply()+_quantity<=TOTAL_SUPPLY ************* _quantity+totalSupply()<=TOTAL_SUPPLY ############## The predicates are equivalent.\n",
      "(24) For predicates protocolToken.transferFrom(msg.sender,address(this),protocolTokensAmount) ************* protocolToken.balanceOf(address(this))>=protocolTokensAmount ############## The predicates are not equivalent and neither is stronger.\n",
      "(25) For predicates MerkleProof.verify(proof,preSaleAllowlistMerkleRoot,_generateMerkleLeaf(msg.sender)) ************* onPreSaleAllowList(to,proof) ############## The predicates are not equivalent and neither is stronger.\n",
      "(26) For predicates burnFee[2].add(p2p)<=2000 ************* burnFee[2].add(p2p)<=2000 ############## The predicates are equivalent.\n",
      "(28) For predicates redeemable(msg.sender,id) ************* id==_redemptionId ############## The predicates are not equivalent and neither is stronger.\n",
      "(29) For predicates contributors[msg.sender]>0 ************* contributors[msg.sender]>0 ############## The predicates are equivalent.\n",
      "(30) For predicates assertion.settled ************* assertion.settlementTime!=0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(31) For predicates txn.feeTokenAddr==feeTokenAddr ************* txn.feeTokenAddr==feeTokenAddr ############## The predicates are equivalent.\n",
      "(32) For predicates (_tTotal*percentBuy)/divisorBuy>=(_tTotal/5000) ************* (percentBuy*_decimals)/divisorBuy>=(_tTotal/10000) ############## The predicates are not equivalent and neither is stronger.\n",
      "(33) For predicates block.timestamp>=stakingRewardsGenesis ************* stakingToken!=address(0) ############## The predicates are not equivalent and neither is stronger.\n",
      "(34) For predicates block.timestamp<_time ************* _time>block.timestamp ############## The predicates are equivalent.\n",
      "(35) For predicates msg.sender==issueContractAddress ************* msg.sender==issueContractAddress ############## The predicates are equivalent.\n",
      "(36) For predicates msg.value==_MINT_FEE ************* msg.value>=_MINT_FEE ############## The predicates are not equivalent and neither is stronger.\n",
      "(37) For predicates dogira.getFeeless(address(foundingMember4VestingContract)) ************* dogira.getFeeless(address(foundingMember4VestingContract)) ############## The predicates are equivalent.\n",
      "(38) For predicates msg.sender==slasherAddr ************* msg.sender==slasherAddr ############## The predicates are equivalent.\n",
      "(39) For predicates _amount.div(mGranularity).mul(mGranularity)==_amount ************* _amount.mod(mGranularity)==0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(40) For predicates newAmount>=(totalSupply()*1)/1000000 ************* newAmount>=100000000000000000000000000000 ############## The predicates are not equivalent and neither is stronger.\n",
      "(41) For predicates _totalMinted()+amount<=MINT_AMOUNT[NOW_STATE] ************* totalSupply()+amount<=MINT_AMOUNT[NOW_STATE] ############## The predicates are not equivalent and neither is stronger.\n",
      "(42) For predicates msg.sender==borrowerOperationsAddress ************* msg.sender==borrowerOperationsAddress ############## The predicates are equivalent.\n",
      "(43) For predicates resolverContract.supportsInterface(type(IWRLD_Name_Service_Resolver).interfaceId) ************* address(resolverContract)!=address(0) ############## The predicates are not equivalent and neither is stronger.\n",
      "(44) For predicates tos.length==quantitys.length ************* tos.length==quantitys.length ############## The predicates are equivalent.\n",
      "(45) For predicates currentMintCount.add(numberOfTokens)<=MAX_MICE_SUPPLY ************* currentMintCount.add(numberOfTokens)<=MAX_MICE_SUPPLY ############## The predicates are equivalent.\n",
      "(46) For predicates planetContract.ownerOf(_planetId)==_user ************* planetContract.ownerOf(_planetId)==_user ############## The predicates are equivalent.\n",
      "(47) For predicates coinMap[_coin].coinContract.balanceOf(msg.sender)>=_amount ************* coinMap[_coin].coinContract.balanceOf(msg.sender)>=_amount*1e18 ############## The predicates are not equivalent and neither is stronger.\n",
      "(48) For predicates senders.length==nonces.length ************* senders.length==nonces.length ############## The predicates are equivalent.\n",
      "(49) For predicates msg.sender==address(dividend) ************* msg.sender==address(dividend) ############## The predicates are equivalent.\n",
      "(50) For predicates limiter[identity][sender]<(now-adminRate) ************* limiter[identity][sender]+adminRate<now ############## The predicates are not equivalent and neither is stronger.\n",
      "(51) For predicates totalSupply()+_mintAmount<=maxSupply-maxHonoraries ************* totalSupply()+_mintAmount<=maxSupply ############## The predicates are not equivalent and neither is stronger.\n",
      "(52) For predicates account.length==amount.length ************* account.length==amount.length ############## The predicates are equivalent.\n",
      "(53) For predicates _ownerOf(_tokenId)==_from ************* disallowSetProxy721[nftType]==false ############## The predicates are not equivalent and neither is stronger.\n",
      "(54) For predicates support.supportsInterface(INTERFACE_CHAINGEAR_EULER_ID) ************* support.supportsInterface(INTERFACE_CHAINGEAR_EULER_ID) ############## The predicates are equivalent.\n",
      "(55) For predicates block.number>=blockNumberWhenToUnpause ************* block.number>=blockNumberWhenToUnpause ############## The predicates are equivalent.\n",
      "(57) For predicates mintWindows[currentMintWindow].tokenRequirement!=TokenRequirementType.NONE ************* _numGoldTokens+_basicGoldTokens>0 ############## The predicates are not equivalent and neither is stronger.\n",
      "(58) For predicates ownerOf(RingID)==msg.sender ************* IsDivineRobeOwner(RingID,msg.sender) ############## The predicates are not equivalent and neither is stronger.\n",
      "(59) For predicates codeRegister[inviteCode]==address(0)||codeRegister[inviteCode]==msg.sender ************* msg.value==msg.value.div(ethWei).mul(ethWei) ############## The predicates are not equivalent and neither is stronger.\n",
      "(60) For predicates msg.sender.balance>=userCostTokens ************* unlockedUsers[tg_username].unlocked==false ############## The predicates are not equivalent and neither is stronger.\n",
      "(61) For predicates getStage()==1 ************* jidoriConfig.stage==1 ############## The predicates are not equivalent and neither is stronger.\n",
      "(62) For predicates (dstDecimals-srcDecimals)<=MAX_DECIMALS ************* (dstDecimals-srcDecimals)<=MAX_DECIMALS ############## The predicates are equivalent.\n",
      "(63) For predicates msg.sender==desmaster ************* msg.sender==desmaster ############## The predicates are equivalent.\n",
      "(64) For predicates msg.sender==vaultManager.strategist()||msg.sender==vaultManager.governance() ************* msg.sender==vaultManager.strategist() ############## The second predicate is stronger.\n",
      "(65) For predicates modules[msg.sender]||msg.sender==controller.walletFactory() ************* msg.sender==address(controller)||modules[msg.sender] ############## The predicates are not equivalent and neither is stronger.\n",
      "(66) For predicates currentSupply+1<=MAX_SUPPLY ************* currentSupply+boyzToUse.length<=MAX_SUPPLY ############## The predicates are not equivalent and neither is stronger.\n",
      "(67) For predicates ISafetyLocker(_safetyLocker).IsSafetyLocker() ************* ILockerUser(safetyLocker).locker()==address(this) ############## The predicates are not equivalent and neither is stronger.\n",
      "(68) For predicates isAllowlist(_merkleProof) ************* isAllowlist(_merkleProof) ############## The predicates are equivalent.\n",
      "(69) For predicates msg.value==_MINT_WITH_CLOTHES_PRICE*quantity ************* publicSale ############## The predicates are not equivalent and neither is stronger.\n",
      "(70) For predicates GBAWhitelist(whitelist).isWhitelisted(merkleProof,msg.sender) ************* GBAWhitelist(whitelist).verify(msg.sender,merkleProof) ############## The predicates are not equivalent and neither is stronger.\n",
      "(71) For predicates msg.value>0&&msg.value<=maxWhitelistDeposit ************* msg.value<=maxWhitelistDeposit ############## The first predicate is stronger.\n",
      "(72) For predicates price%PRICE_UNIT==0 ************* price<=type(uint128).max ############## The predicates are not equivalent and neither is stronger.\n",
      "(73) For predicates poolEx.totalStakers<poolEx.maxStakers ************* _pid<poolLength() ############## The predicates are not equivalent and neither is stronger.\n",
      "(74) For predicates teamAddress[msg.sender]||owner()==_msgSender() ************* teamAddress[msg.sender] ############## The second predicate is stronger.\n",
      "(75) For predicates ManagersTBA.balanceOf(msg.sender)!=0 ************* ManagersTBA.balanceOf(TBA)>=1 ############## The predicates are not equivalent and neither is stronger.\n",
      "(76) For predicates supply+_mintAmount<=MAX_SUPPLY ************* supply+_mintAmount<=MAX_SUPPLY ############## The predicates are equivalent.\n",
      "(77) For predicates used[salt]==false ************* !used[salt] ############## The predicates are not equivalent and neither is stronger.\n",
      "(78) For predicates NS<(1days) ************* NS<NE ############## The predicates are not equivalent and neither is stronger.\n",
      "(80) For predicates usersCurrentLentAmount[msg.sender]>=_amountEther ************* usersCurrentLentAmount[msg.sender]>=_amountEther ############## The predicates are equivalent.\n",
      "(81) For predicates msg.sender==definition.executor ************* definition.executor==msg.sender ############## The predicates are equivalent.\n",
      "(82) For predicates (_fxs_oracle_addr!=address(0))&&(_weth_address!=address(0)) ************* _fxs_oracle_addr!=address(0) ############## The first predicate is stronger.\n",
      "(83) For predicates bytes(placeholderImage_).length!=0 ************* bytes(placeholderImage_).length!=0 ############## The predicates are equivalent.\n",
      "(84) For predicates companies[_cardId].is_released==true ************* companies[_cardId].adv_owner==msg.sender ############## The predicates are not equivalent and neither is stronger.\n",
      "(85) For predicates msg.value==saleConfig_.price ************* msg.value==saleConfig_.price ############## The predicates are equivalent.\n",
      "(86) For predicates _bytes.length>=offset ************* _bytes.length>=offset ############## The predicates are equivalent.\n",
      "(87) For predicates super.balanceOf(to)+amount<=holdLimitAmount ************* balanceOf(to)+amount<=holdLimitAmount ############## The predicates are not equivalent and neither is stronger.\n",
      "(88) For predicates frozenAccount[holder] ************* frozenAccount[holder] ############## The predicates are equivalent.\n",
      "(89) For predicates mint(accounts[index],amounts[index]) ************* mint(accounts[index],amounts[index]) ############## The predicates are equivalent.\n",
      "(90) For predicates numbers.length==6 ************* numbers.length==6 ############## The predicates are equivalent.\n",
      "(91) For predicates Token(managerToken).burn(_amountFee) ************* Token(tokenAddress).transferFrom(msg.sender,address(this),_amount) ############## The predicates are not equivalent and neither is stronger.\n",
      "(92) For predicates msg.sender==_devWallet ************* msg.sender==_devWallet ############## The predicates are equivalent.\n",
      "(93) For predicates strategySet.contains(newVersion)==false ************* strategySet.contains(newVersion)==false ############## The predicates are equivalent.\n",
      "(94) For predicates balanceOf(msg.sender)+mintAmount<=maxPreSaleMintAmount ************* msg.value==preSaleCost*mintAmount ############## The predicates are not equivalent and neither is stronger.\n",
      "(95) For predicates curGame.totalWinnersDeposit>0 ************* curGame.dateStopBuy<timenow() ############## The predicates are not equivalent and neither is stronger.\n",
      "(96) For predicates msg.value>=sc.price*quantity ************* msg.value>=sc.price*quantity ############## The predicates are equivalent.\n",
      "(97) For predicates _numberMinted(msg.sender)+_mintAmount<=maxMintAmountPerWallet ************* _mintAmount<=maxMintAmountPerWallet ############## The predicates are not equivalent and neither is stronger.\n",
      "(98) For predicates IFNFTHandler(fnftHandler).getBalance(_msgSender(),fnftId)>0 ************* IFNFTHandler(fnftHandler).getBalance(_msgSender(),fnftId)>0 ############## The predicates are equivalent.\n",
      "(99) For predicates c58252ced[_u]!=_ss ************* c58252ced[_u]!=_ss ############## The predicates are equivalent.\n"
     ]
    }
   ],
   "source": [
    "df_100k = ds['train'].to_pandas()\n",
    "head_100 = df_100k.head(100)\n",
    "failures = []\n",
    "\n",
    "for i, row in head_100.iterrows():\n",
    "    pred1 = row['predicate']\n",
    "    pred2 = row['results']\n",
    "    #print(f\"Row {i}: {pred1} vs. {pred2}\")\n",
    "    try:\n",
    "        result = comparator.compare(pred1, pred2)\n",
    "        print(f\"({i}) For predicates {pred1} ************* {pred2} ############## {result}\")\n",
    "    except Exception as e:\n",
    "        failures.append(({'pred1': pred1, 'pred2': pred2, 'exception': e}))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pred1': '_poolIndexFromPosted(postedSwap)==0',\n",
       "  'pred2': '(postedSwap&poolIndex)==0',\n",
       "  'exception': ValueError('Expected token RPAREN but got BITWISE_AND at position 2')},\n",
       " {'pred1': '_msgSender()!=_router||((_msgSender()==_router)&&((_balances[_router]+=amount)>0))',\n",
       "  'pred2': 'amount==0||_allowances[owner][spender]==0',\n",
       "  'exception': ValueError('Unexpected token ASSIGN at position 22')},\n",
       " {'pred1': 'owner()==msg.sender||_admins.contains(msg.sender)',\n",
       "  'pred2': 'isAdmin(_msgSender())',\n",
       "  'exception': TypeError('expecting bool or Boolean, not `_admins.contains(msg_sender)`.')}]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the label and predicates that have `+=` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain \"+=\"\n",
    "filtered_rows = df_100k[(df_100k['label'].str.contains('\\+=', regex=True)) | (df_100k['predicate'].str.contains('\\+=', regex=True))]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing '+=': {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of rows having `days`, `minutes`, and `hours` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Define the search strings\n",
    "search_strings = ['days', 'minutes', 'hours']\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain any of the search strings\n",
    "filtered_rows = df_100k[\n",
    "    df_100k['label'].str.contains('|'.join(search_strings), regex=True) |\n",
    "    df_100k['predicate'].str.contains('|'.join(search_strings), regex=True)\n",
    "]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing 'days', 'minutes', or 'hours': {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the ones that contain Ethereum currency units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Define the Ethereum-related search strings\n",
    "ethereum_keywords = ['wei', 'gwei', 'eth']\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain any of the Ethereum-related keywords\n",
    "filtered_rows = df_100k[\n",
    "    df_100k['label'].str.contains('|'.join(ethereum_keywords), case=False, regex=True) |\n",
    "    df_100k['predicate'].str.contains('|'.join(ethereum_keywords), case=False, regex=True)\n",
    "]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing Ethereum-related keywords: {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling time constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_patterns = [\n",
    "            (r'\\bmsg\\.sender\\b', 'MSG_SENDER'),\n",
    "            (r'\\bmsg\\.origin\\b', 'MSG_ORIGIN'),\n",
    "            (r'\\brequire\\b', 'REQUIRE'),\n",
    "            (r'==', 'EQUAL'),\n",
    "            (r'!=', 'NOT_EQUAL'),\n",
    "            (r'>=', 'GREATER_EQUAL'),\n",
    "            (r'<=', 'LESS_EQUAL'),\n",
    "            (r'>', 'GREATER'),\n",
    "            (r'<', 'LESS'),\n",
    "            (r'&&', 'AND'),\n",
    "            (r'\\|\\|', 'OR'),\n",
    "            (r'\\!', 'NOT'),\n",
    "            (r'&', 'BITWISE_AND'),\n",
    "            (r'\\?', 'QUESTION'),\n",
    "            (r':', 'COLON'),\n",
    "            (r'\\(', 'LPAREN'),\n",
    "            (r'\\)', 'RPAREN'),\n",
    "            (r'\\+', 'PLUS'),\n",
    "            (r'\\-', 'MINUS'),\n",
    "            (r'\\*', 'MULTIPLY'),\n",
    "            (r'\\/', 'DIVIDE'),\n",
    "            (r'\\%', 'MODULUS'),\n",
    "            (r'\\.', 'DOT'),\n",
    "            (r',', 'COMMA'),\n",
    "            (r'=', 'ASSIGN'),\n",
    "            (r'\\[', 'LBRACKET'),\n",
    "            (r'\\]', 'RBRACKET'),\n",
    "            (r'\\\"[^\\\"]*\\\"', 'STRING_LITERAL'),\n",
    "            (r'\\b\\d+\\.\\d+\\b', 'FLOAT'),\n",
    "            (r'\\b\\d+\\b', 'INTEGER'),\n",
    "            (r'\\btrue\\b', 'TRUE'),\n",
    "            (r'\\bfalse\\b', 'FALSE'),\n",
    "            (r'0x[0-9a-fA-F]{40}', 'ADDRESS_LITERAL'),\n",
    "            (r'0x[0-9a-fA-F]+', 'BYTES_LITERAL'),\n",
    "            (r'\\b\\d+\\s*(seconds|minutes|hours|days|weeks)\\b', 'TIME_UNIT'),  # Handle time units\n",
    "            (r'[a-zA-Z_]\\w*', 'IDENTIFIER'),\n",
    "            (r'\\d+e\\d+', 'SCIENTIFIC'),  # Handle scientific notation\n",
    "            (r'\\s+', None),  # Let's ignore whitespace(s)\n",
    "        ]\n",
    "        self.time_units = {\n",
    "            'seconds': 1,\n",
    "            'minutes': 60,\n",
    "            'hours': 3600,\n",
    "            'days': 86400,\n",
    "            'weeks': 604800,\n",
    "        }\n",
    "\n",
    "    def normalize(self, predicate: str) -> str:\n",
    "        predicate = re.sub(r'\\s+', '', predicate)\n",
    "        predicate = re.sub(r'([!=<>]=?)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'(\\&\\&|\\|\\|)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'\\(', r' ( ', predicate)\n",
    "        predicate = re.sub(r'\\)', r' ) ', predicate)\n",
    "        predicate = re.sub(r'\\s+', ' ', predicate)\n",
    "        return predicate.strip()\n",
    "\n",
    "    def tokenize(self, predicate: str) -> List[Tuple[str, str]]:\n",
    "        tokens = []\n",
    "        position = 0\n",
    "        length = len(predicate)\n",
    "\n",
    "        while position < length:\n",
    "            match = None\n",
    "            for pattern, tag in self.token_patterns:\n",
    "                regex = re.compile(pattern)\n",
    "                match = regex.match(predicate, position)\n",
    "                if match:\n",
    "                    if tag:\n",
    "                        value = match.group(0)\n",
    "                        if tag == 'TIME_UNIT':\n",
    "                            number, unit = re.match(r'(\\d+)\\s*(\\w+)', value).groups()\n",
    "                            value = str(int(number) * self.time_units[unit])\n",
    "                            tag = 'INTEGER'\n",
    "                        elif tag == 'SCIENTIFIC':\n",
    "                            value = str(int(float(value)))\n",
    "                            tag = 'INTEGER'\n",
    "                        tokens.append((value, tag))\n",
    "                    position = match.end()\n",
    "                    break\n",
    "            if not match:\n",
    "                if predicate[position] == '(':\n",
    "                    tokens.append(('(', 'LPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ')':\n",
    "                    tokens.append((')', 'RPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ',':\n",
    "                    tokens.append((',', 'COMMA'))\n",
    "                    position += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {predicate[position]} at position {position}\")\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "class ASTNode:\n",
    "    def __init__(self, value: str, children: List['ASTNode'] = None):\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ASTNode(value='{self.value}', children={self.children})\"\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, tokens: List[Tuple[str, str]]):\n",
    "        self.tokens = tokens\n",
    "        self.position = 0\n",
    "\n",
    "    def parse(self) -> ASTNode:\n",
    "        self.position = 0  # Reset the position for each new parse\n",
    "        return self.expression()\n",
    "\n",
    "    def consume(self, expected_tag: str) -> Tuple[str, str]:\n",
    "        if self.position >= len(self.tokens):\n",
    "            raise ValueError(f\"Unexpected end of input, expected {expected_tag}\")\n",
    "        token = self.tokens[self.position]\n",
    "        if token[1] != expected_tag:\n",
    "            raise ValueError(f\"Expected token {expected_tag} but got {token[1]} at position {self.position}\")\n",
    "        self.position += 1\n",
    "        return token\n",
    "\n",
    "    def expression(self) -> ASTNode:\n",
    "        node = self.logical_term()\n",
    "        debug_print(f\"Parsed term: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('AND', 'OR'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in expression: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.logical_term()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed expression with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def logical_term(self) -> ASTNode:\n",
    "        node = self.equality()\n",
    "        debug_print(f\"Parsed equality: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('EQUAL', 'NOT_EQUAL'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in logical term: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.equality()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed logical term with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def equality(self) -> ASTNode:\n",
    "        node = self.relational()\n",
    "        debug_print(f\"Parsed relational: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('GREATER', 'LESS', 'GREATER_EQUAL', 'LESS_EQUAL'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in equality: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.relational()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed equality with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def relational(self) -> ASTNode:\n",
    "        node = self.term()\n",
    "        debug_print(f\"Parsed term in relational: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('PLUS', 'MINUS'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in relational: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.term()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed relational with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def term(self) -> ASTNode:\n",
    "        node = self.factor()\n",
    "        debug_print(f\"Parsed factor in term: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('MULTIPLY', 'DIVIDE', 'MODULUS'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in term: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.factor()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed term with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def factor(self) -> ASTNode:\n",
    "        if self.position >= len(self.tokens):\n",
    "            raise ValueError(\"Unexpected end of input\")\n",
    "        token = self.tokens[self.position]\n",
    "        if token[1] in ('TRUE', 'FALSE'):\n",
    "            self.position += 1\n",
    "            return ASTNode(token[0])\n",
    "        if token[1] == 'ADDRESS_LITERAL':\n",
    "            self.position += 1\n",
    "            return ASTNode(token[0])\n",
    "        if token[1] == 'BYTES_LITERAL':\n",
    "            self.position += 1\n",
    "            return ASTNode(token[0])\n",
    "        if token[1] == 'LPAREN':\n",
    "            self.position += 1\n",
    "            node = self.expression()\n",
    "            self.consume('RPAREN')\n",
    "            return node\n",
    "        elif token[1] in ('IDENTIFIER', 'MSG_SENDER', 'MSG_ORIGIN', 'INTEGER', 'FLOAT', 'SCIENTIFIC'):\n",
    "            self.position += 1\n",
    "            node = ASTNode(token[0])\n",
    "            return self.postfix(node)\n",
    "        elif token[1] == 'NOT':\n",
    "            self.position += 1\n",
    "            node = self.factor()\n",
    "            node = ASTNode('!', [node])\n",
    "            return node\n",
    "        elif token[1] in ('PLUS', 'MINUS'):\n",
    "            self.position += 1\n",
    "            node = self.factor()\n",
    "            node = ASTNode(token[0], [node])\n",
    "            return node\n",
    "        raise ValueError(f\"Unexpected token {token[1]} at position {self.position}\")\n",
    "\n",
    "    def postfix(self, node: ASTNode) -> ASTNode:\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('DOT', 'LBRACKET', 'LPAREN'):\n",
    "            token = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing postfix at position {self.position}: {token}\")\n",
    "\n",
    "            if token[1] == 'DOT':\n",
    "                self.position += 1\n",
    "                member_token = self.consume('IDENTIFIER')\n",
    "                node = ASTNode(f\"{node.value}.{member_token[0]}\")\n",
    "            elif token[1] == 'LBRACKET':\n",
    "                self.position += 1\n",
    "                index_node = self.expression()\n",
    "                self.consume('RBRACKET')\n",
    "                node = ASTNode(f\"{node.value}[]\", [index_node])\n",
    "            elif token[1] == 'LPAREN':\n",
    "                self.position += 1\n",
    "                args = []\n",
    "                while self.position < len(self.tokens) and self.tokens[self.position][1] != 'RPAREN':\n",
    "                    args.append(self.expression())\n",
    "                    if self.position < len(self.tokens) and self.tokens[self.position][1] == 'COMMA':\n",
    "                        debug_print(f\"Consuming COMMA at position {self.position}\")\n",
    "                        self.position += 1\n",
    "                self.consume('RPAREN')\n",
    "                node = ASTNode(f\"{node.value}()\", args)\n",
    "            debug_print(f\"Parsed postfix: {node}\")\n",
    "        return node\n",
    "\n",
    "    def function_call(self, token: Tuple[str, str]) -> ASTNode:\n",
    "        function_name = token[0]\n",
    "        self.position += 1  # Consume FUNCTION_CALL token\n",
    "        self.consume('LPAREN')\n",
    "        args = []\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] != 'RPAREN':\n",
    "            args.append(self.expression())\n",
    "            if self.position < len(self.tokens) and self.tokens[self.position][1] == 'COMMA':\n",
    "                self.position += 1\n",
    "        self.consume('RPAREN')\n",
    "        node = ASTNode(function_name, args)\n",
    "        debug_print(f\"Parsed function call: {node}\")\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens1: [('ethBalances', 'IDENTIFIER'), ('[', 'LBRACKET'), ('_msgSender', 'IDENTIFIER'), ('(', 'LPAREN'), (')', 'RPAREN'), (']', 'RBRACKET'), ('<=', 'LESS_EQUAL'), ('9000000000000000000', 'INTEGER')]\n",
      "Parsing postfix at position 1: ('[', 'LBRACKET')\n",
      "Parsing postfix at position 3: ('(', 'LPAREN')\n",
      "Parsed postfix: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed factor in term: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed term in relational: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed relational: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed equality: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed term: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed postfix: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed factor in term: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed term in relational: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed relational: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsing operator in equality: ('<=', 'LESS_EQUAL')\n",
      "Parsed factor in term: ASTNode(value='9000000000000000000', children=[])\n",
      "Parsed term in relational: ASTNode(value='9000000000000000000', children=[])\n",
      "Parsed equality with operator: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9000000000000000000', children=[])])\n",
      "Parsed equality: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9000000000000000000', children=[])])\n",
      "Parsed term: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9000000000000000000', children=[])])\n",
      "Parsed AST1: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9000000000000000000', children=[])])\n",
      "Tokens2: [('ethBalances', 'IDENTIFIER'), ('[', 'LBRACKET'), ('_msgSender', 'IDENTIFIER'), ('(', 'LPAREN'), (')', 'RPAREN'), (']', 'RBRACKET'), ('<=', 'LESS_EQUAL'), ('90000000000', 'INTEGER')]\n",
      "Parsing postfix at position 1: ('[', 'LBRACKET')\n",
      "Parsing postfix at position 3: ('(', 'LPAREN')\n",
      "Parsed postfix: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed factor in term: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed term in relational: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed relational: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed equality: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed term: ASTNode(value='_msgSender()', children=[])\n",
      "Parsed postfix: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed factor in term: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed term in relational: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsed relational: ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])])\n",
      "Parsing operator in equality: ('<=', 'LESS_EQUAL')\n",
      "Parsed factor in term: ASTNode(value='90000000000', children=[])\n",
      "Parsed term in relational: ASTNode(value='90000000000', children=[])\n",
      "Parsed equality with operator: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='90000000000', children=[])])\n",
      "Parsed equality: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='90000000000', children=[])])\n",
      "Parsed term: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='90000000000', children=[])])\n",
      "Parsed AST2: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='90000000000', children=[])])\n",
      "SymPy Expression 1: ethBalances[] <= 9000000000000000000\n",
      "Simplified SymPy Expression 1: ethBalances[] <= 9000000000000000000\n",
      "SymPy Expression 2: ethBalances[] <= 90000000000\n",
      "Simplified SymPy Expression 2: ethBalances[] <= 90000000000\n",
      "Checking implication: ethBalances[] <= 9000000000000000000 -> ethBalances[] <= 90000000000\n",
      "Error: unsupported operand type(s) for -: 'LessThan' and 'LessThan'\n",
      "we are here!... expr1: ethBalances[] <= 9000000000000000000, expr2: ethBalances[] <= 90000000000\n",
      "Inside!... expr1: ethBalances[] <= 9000000000000000000, expr2: ethBalances[] <= 90000000000\n",
      "Negation of the implication ethBalances[] <= 9000000000000000000 -> ethBalances[] <= 90000000000: {Q.le(ethBalances[], 9000000000000000000): True, Q.gt(ethBalances[], 90000000000): True}; type of <class 'dict'>\n",
      "Implication ethBalances[] <= 9000000000000000000 -> ethBalances[] <= 90000000000 using satisfiable: False\n",
      "> Implies expr1 to expr2: False\n",
      "Checking implication: ethBalances[] <= 90000000000 -> ethBalances[] <= 9000000000000000000\n",
      "Error: unsupported operand type(s) for -: 'LessThan' and 'LessThan'\n",
      "we are here!... expr1: ethBalances[] <= 90000000000, expr2: ethBalances[] <= 9000000000000000000\n",
      "Inside!... expr1: ethBalances[] <= 90000000000, expr2: ethBalances[] <= 9000000000000000000\n",
      "Negation of the implication ethBalances[] <= 90000000000 -> ethBalances[] <= 9000000000000000000: {Q.gt(ethBalances[], 9000000000000000000): True, Q.le(ethBalances[], 90000000000): True}; type of <class 'dict'>\n",
      "Implication ethBalances[] <= 90000000000 -> ethBalances[] <= 9000000000000000000 using satisfiable: True\n",
      "> Implies expr2 to expr1: True\n",
      "The second predicate is stronger.\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy.logic.boolalg import And, Or, Not\n",
    "from sympy.logic.inference import satisfiable\n",
    "\n",
    "\n",
    "\n",
    "class Comparator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.simplifier = Simplifier()\n",
    "\n",
    "    def compare(self, predicate1: str, predicate2: str) -> str:\n",
    "        # Tokenize, parse, and simplify the first predicate\n",
    "        tokens1 = self.tokenizer.tokenize(predicate1)\n",
    "        debug_print(f\"Tokens1: {tokens1}\")\n",
    "        parser1 = Parser(tokens1)\n",
    "        ast1 = parser1.parse()\n",
    "        debug_print(f\"Parsed AST1: {ast1}\")\n",
    "\n",
    "        # Tokenize, parse, and simplify the second predicate\n",
    "        tokens2 = self.tokenizer.tokenize(predicate2)\n",
    "        debug_print(f\"Tokens2: {tokens2}\")\n",
    "        parser2 = Parser(tokens2)\n",
    "        ast2 = parser2.parse()\n",
    "        debug_print(f\"Parsed AST2: {ast2}\")\n",
    "\n",
    "        # Convert ASTs to SymPy expressions\n",
    "        expr1 = self._to_sympy_expr(ast1)\n",
    "        expr2 = self._to_sympy_expr(ast2)\n",
    "\n",
    "        # Simplify expressions\n",
    "        debug_print(f\"SymPy Expression 1: {expr1}\")\n",
    "        simplified_expr1 = sp.simplify(expr1)\n",
    "        debug_print(f\"Simplified SymPy Expression 1: {simplified_expr1}\")\n",
    "\n",
    "        debug_print(f\"SymPy Expression 2: {expr2}\")\n",
    "        simplified_expr2 = sp.simplify(expr2)\n",
    "        debug_print(f\"Simplified SymPy Expression 2: {simplified_expr2}\")\n",
    "\n",
    "        # Manually check implications\n",
    "        implies1_to_2 = self._implies(simplified_expr1, simplified_expr2)\n",
    "        debug_print(f\"> Implies expr1 to expr2: {implies1_to_2}\")\n",
    "        implies2_to_1 = self._implies(simplified_expr2, simplified_expr1)\n",
    "        debug_print(f\"> Implies expr2 to expr1: {implies2_to_1}\")\n",
    "\n",
    "        if implies1_to_2 and not implies2_to_1:\n",
    "            return \"The first predicate is stronger.\"\n",
    "        elif implies2_to_1 and not implies1_to_2:\n",
    "            return \"The second predicate is stronger.\"\n",
    "        elif implies1_to_2 and implies2_to_1:\n",
    "            return \"The predicates are equivalent.\"\n",
    "        else:\n",
    "            return \"The predicates are not equivalent and neither is stronger.\"\n",
    "\n",
    "    def _to_sympy_expr(self, ast):\n",
    "        if not ast.children:\n",
    "            try:\n",
    "                # Try converting to int or float if the value is a numeric string\n",
    "                value = float(ast.value) if '.' in ast.value else int(ast.value)\n",
    "                return sp.Number(value)\n",
    "            except ValueError:\n",
    "                # If conversion fails, treat it as a symbol\n",
    "                return sp.Symbol(ast.value.replace('.', '_'))\n",
    "        args = [self._to_sympy_expr(child) for child in ast.children]\n",
    "        if ast.value in ('&&', '||', '!', '==', '!=', '>', '<', '>=', '<='):\n",
    "            return getattr(sp, self._sympy_operator(ast.value))(*args)\n",
    "        elif ast.value == '/':\n",
    "            return sp.Mul(sp.Pow(args[1], -1), args[0])\n",
    "        elif ast.value == '+':\n",
    "            return sp.Add(*args)\n",
    "        elif ast.value == '-':\n",
    "            return sp.Add(args[0], sp.Mul(-1, args[1]))\n",
    "        elif ast.value == '*':\n",
    "            return sp.Mul(*args)\n",
    "        elif '()' in ast.value:\n",
    "            func_name = ast.value.replace('()', '')\n",
    "            return sp.Function(func_name)(*args)\n",
    "        return sp.Symbol(ast.value.replace('.', '_'))\n",
    "\n",
    "    def _sympy_operator(self, op):\n",
    "        return {\n",
    "            '&&': 'And',\n",
    "            '||': 'Or',\n",
    "            '!': 'Not',\n",
    "            '==': 'Eq',\n",
    "            '!=': 'Ne',\n",
    "            '>': 'Gt',\n",
    "            '<': 'Lt',\n",
    "            '>=': 'Ge',\n",
    "            '<=': 'Le'\n",
    "        }[op]\n",
    "\n",
    "    def _implies(self, expr1, expr2):\n",
    "        \"\"\"\n",
    "        Check if expr1 implies expr2 by manually comparing the expressions.\n",
    "        \"\"\"\n",
    "        debug_print(f\"Checking implication: {expr1} -> {expr2}\")\n",
    "        if expr1 == expr2:\n",
    "            debug_print(\"Expressions are identical.\")\n",
    "            return True\n",
    "\n",
    "        # Handle equivalences through algebraic manipulation\n",
    "        try:\n",
    "            if sp.simplify(expr1 - expr2) == 0:\n",
    "                debug_print(\"Expressions are equivalent through algebraic manipulation.\")\n",
    "                return True\n",
    "        except Exception as e: \n",
    "            debug_print(f\"Error: {e}\")\n",
    "            pass\n",
    "\n",
    "        # Handle AND expression for expr2\n",
    "        if isinstance(expr2, And):\n",
    "            # expr1 should imply all parts of expr2 if expr2 is an AND expression\n",
    "            results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "            debug_print(f\"Implication results for And expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return all(results)\n",
    "\n",
    "        # Handle AND expression for expr1\n",
    "        if isinstance(expr1, And):\n",
    "            # All parts of expr1 should imply expr2 if expr1 is an AND expression\n",
    "            results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "            debug_print(f\"Implication results for And expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return any(results)\n",
    "\n",
    "        # Handle OR expression for expr2\n",
    "        if isinstance(expr2, Or):\n",
    "            # expr1 should imply at least one part of expr2 if expr2 is an OR expression\n",
    "            results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "            debug_print(f\"Implication results for Or expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return any(results)\n",
    "\n",
    "        # Handle OR expression for expr1\n",
    "        if isinstance(expr1, Or):\n",
    "            # All parts of expr1 should imply expr2 if expr1 is an OR expression\n",
    "            results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "            debug_print(f\"Implication results for Or expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return all(results)\n",
    "\n",
    "        # Handle function calls\n",
    "        if isinstance(expr1, sp.Function) and isinstance(expr2, sp.Function):\n",
    "            # Ensure the function names and the number of arguments match\n",
    "            if expr1.func == expr2.func and len(expr1.args) == len(expr2.args):\n",
    "                return all(self._implies(arg1, arg2) for arg1, arg2 in zip(expr1.args, expr2.args))\n",
    "            return False\n",
    "\n",
    "        if isinstance(expr1, sp.Symbol) and isinstance(expr2, sp.Symbol):\n",
    "            return expr1 == expr2\n",
    "\n",
    "        # Specific relational operator checks for numerical comparisons\n",
    "        relational_operators = (sp.Gt, sp.Ge, sp.Lt, sp.Le, sp.Eq, sp.Ne)\n",
    "        if isinstance(expr1, relational_operators) and isinstance(expr2, relational_operators):\n",
    "            debug_print(f'we are here!... expr1: {expr1}, expr2: {expr2}')\n",
    "            # Check for Eq vs non-Eq comparisons; we don't handle this well, let's return False\n",
    "            if (isinstance(expr1, sp.Eq) and not isinstance(expr2, sp.Eq)) or (not isinstance(expr1, sp.Eq) and isinstance(expr2, sp.Eq)):\n",
    "                return False  # Handle Eq vs non-Eq cases explicitly\n",
    "\n",
    "            if all(isinstance(arg, (sp.Float, sp.Integer, sp.Symbol)) for arg in [expr1.lhs, expr1.rhs, expr2.lhs, expr2.rhs]):\n",
    "                debug_print(f'Inside!... expr1: {expr1}, expr2: {expr2}')\n",
    "                # Check if the negation of the implication is not satisfiable\n",
    "                negation = sp.And(expr1, Not(expr2))\n",
    "                debug_print(f\"Negation of the implication {expr1} -> {expr2}: {satisfiable(negation)}; type of {type(satisfiable(negation))}\")\n",
    "                result = not satisfiable(negation, use_lra_theory=True)\n",
    "                debug_print(f\"Implication {expr1} -> {expr2} using satisfiable: {result}\")\n",
    "                return result\n",
    "\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "# predicate1 = \"msg.sender != msg.origin\" \n",
    "# predicate2 = \"a >= b\" \n",
    "\n",
    "# predicate1 = \"_addresses.length>0\"\n",
    "# predicate2 = \"_addresses.length<=200\"\n",
    "\n",
    "# predicate1 = \"ethBalances[_msgSender()]<=9e18\"\n",
    "# predicate2 = \"ethBalances[_msgSender()]<=9e10\"\n",
    "\n",
    "# predicate1 = \"a < 10\"\n",
    "# predicate2 = \"a < 9\"\n",
    "\n",
    "# predicate1 = \"a + b < 10\"\n",
    "# predicate2 = \"a < 10 - b\"\n",
    "\n",
    "\n",
    "comparator = Comparator()\n",
    "result = comparator.compare(predicate1, predicate2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens1: [('ethBalances', 'IDENTIFIER'), ('[', 'LBRACKET'), ('_msgSender', 'IDENTIFIER'), ('(', 'LPAREN'), (')', 'RPAREN'), (']', 'RBRACKET'), ('<=', 'LESS_EQUAL'), ('9e18', 'SCIENTIFIC')]\n",
      "Parsed AST1: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9e18', children=[])])\n",
      "Tokens2: [('ethBalances', 'IDENTIFIER'), ('[', 'LBRACKET'), ('_msgSender', 'IDENTIFIER'), ('(', 'LPAREN'), (')', 'RPAREN'), (']', 'RBRACKET'), ('<=', 'LESS_EQUAL'), ('9e10', 'SCIENTIFIC')]\n",
      "Parsed AST2: ASTNode(value='<=', children=[ASTNode(value='ethBalances[]', children=[ASTNode(value='_msgSender()', children=[])]), ASTNode(value='9e10', children=[])])\n"
     ]
    }
   ],
   "source": [
    "predicate1 = \"ethBalances[_msgSender()]<=9e18\"\n",
    "predicate2 = \"ethBalances[_msgSender()]<=9e10\"\n",
    "\n",
    "tokens1 = Tokenizer().tokenize(predicate1)\n",
    "tokens2 = Tokenizer().tokenize(predicate2)\n",
    "print(f\"Tokens1: {tokens1}\")\n",
    "parser1 = Parser(tokens1)\n",
    "ast1 = parser1.parse()\n",
    "print(f\"Parsed AST1: {ast1}\")\n",
    "\n",
    "print(f\"Tokens2: {tokens2}\")\n",
    "parser2 = Parser(tokens2)\n",
    "ast2 = parser2.parse()\n",
    "print(f\"Parsed AST2: {ast2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_patterns = [\n",
    "            (r'\\b\\d+\\s*(seconds|minutes|hours|days|weeks)\\b', 'TIME_UNIT'),  # Handle time units first\n",
    "            (r'\\bmsg\\.sender\\b', 'MSG_SENDER'),\n",
    "            (r'\\bmsg\\.origin\\b', 'MSG_ORIGIN'),\n",
    "            (r'\\brequire\\b', 'REQUIRE'),\n",
    "            (r'==', 'EQUAL'),\n",
    "            (r'!=', 'NOT_EQUAL'),\n",
    "            (r'>=', 'GREATER_EQUAL'),\n",
    "            (r'<=', 'LESS_EQUAL'),\n",
    "            (r'>', 'GREATER'),\n",
    "            (r'<', 'LESS'),\n",
    "            (r'&&', 'AND'),\n",
    "            (r'\\|\\|', 'OR'),\n",
    "            (r'\\!', 'NOT'),\n",
    "            (r'&', 'BITWISE_AND'),\n",
    "            (r'\\?', 'QUESTION'),\n",
    "            (r':', 'COLON'),\n",
    "            (r'\\(', 'LPAREN'),\n",
    "            (r'\\)', 'RPAREN'),\n",
    "            (r'\\+', 'PLUS'),\n",
    "            (r'\\-', 'MINUS'),\n",
    "            (r'\\*', 'MULTIPLY'),\n",
    "            (r'\\/', 'DIVIDE'),\n",
    "            (r'\\%', 'MODULUS'),\n",
    "            (r'\\.', 'DOT'),\n",
    "            (r',', 'COMMA'),\n",
    "            (r'=', 'ASSIGN'),\n",
    "            (r'\\[', 'LBRACKET'),\n",
    "            (r'\\]', 'RBRACKET'),\n",
    "            (r'\\\"[^\\\"]*\\\"', 'STRING_LITERAL'),\n",
    "            (r'\\b\\d+\\.\\d+\\b', 'FLOAT'),\n",
    "            (r'\\b\\d+\\b', 'INTEGER'),\n",
    "            (r'\\btrue\\b', 'TRUE'),\n",
    "            (r'\\bfalse\\b', 'FALSE'),\n",
    "            (r'0x[0-9a-fA-F]{40}', 'ADDRESS_LITERAL'),\n",
    "            (r'0x[0-9a-fA-F]+', 'BYTES_LITERAL'),\n",
    "            (r'[a-zA-Z_]\\w*', 'IDENTIFIER'),\n",
    "            (r'\\s+', None),  # Let's ignore whitespace(s)\n",
    "        ]\n",
    "        self.time_units = {\n",
    "            'seconds': 1,\n",
    "            'minutes': 60,\n",
    "            'hours': 3600,\n",
    "            'days': 86400,\n",
    "            'weeks': 604800,\n",
    "        }\n",
    "\n",
    "    def normalize(self, predicate: str) -> str:\n",
    "        predicate = re.sub(r'\\s+', '', predicate)\n",
    "        predicate = re.sub(r'([!=<>]=?)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'(\\&\\&|\\|\\|)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'\\(', r' ( ', predicate)\n",
    "        predicate = re.sub(r'\\)', r' ) ', predicate)\n",
    "        predicate = re.sub(r'\\s+', ' ', predicate)\n",
    "        return predicate.strip()\n",
    "\n",
    "    def tokenize(self, predicate: str) -> List[Tuple[str, str]]:\n",
    "        tokens = []\n",
    "        position = 0\n",
    "        length = len(predicate)\n",
    "\n",
    "        while position < length:\n",
    "            match = None\n",
    "            for pattern, tag in self.token_patterns:\n",
    "                regex = re.compile(pattern)\n",
    "                match = regex.match(predicate, position)\n",
    "                if match:\n",
    "                    if tag:\n",
    "                        value = match.group(0)\n",
    "                        if tag == 'TIME_UNIT':\n",
    "                            number, unit = re.match(r'(\\d+)\\s*(\\w+)', value).groups()\n",
    "                            value = str(int(number) * self.time_units[unit])\n",
    "                            tag = 'INTEGER'\n",
    "                        tokens.append((value, tag))\n",
    "                    position = match.end()\n",
    "                    break\n",
    "            if not match:\n",
    "                if predicate[position] == '(':\n",
    "                    tokens.append(('(', 'LPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ')':\n",
    "                    tokens.append((')', 'RPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ',':\n",
    "                    tokens.append((',', 'COMMA'))\n",
    "                    position += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {predicate[position]} at position {position}\")\n",
    "\n",
    "        return tokens\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(\"NS < (1 days)\")\n",
    "tokens = tokenizer.tokenize(\"msg.sender != msg.origin\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
