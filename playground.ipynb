{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sympy==1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(message):\n",
    "    print(f\"{message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy.logic.boolalg import And, Or, Not\n",
    "from sympy.logic.inference import satisfiable\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.parser import Parser\n",
    "from src.simplifier import Simplifier\n",
    "from src.config import debug_print\n",
    "\n",
    "class Comparator:\n",
    "   def __init__(self):\n",
    "       self.tokenizer = Tokenizer()\n",
    "       self.simplifier = Simplifier()\n",
    "\n",
    "   def compare(self, predicate1: str, predicate2: str) -> str:\n",
    "       # Tokenize, parse, and simplify the first predicate\n",
    "       tokens1 = self.tokenizer.tokenize(predicate1)\n",
    "       debug_print(f\"Tokens1: {tokens1}\")\n",
    "       parser1 = Parser(tokens1)\n",
    "       ast1 = parser1.parse()\n",
    "       debug_print(f\"Parsed AST1: {ast1}\")\n",
    "\n",
    "       # Tokenize, parse, and simplify the second predicate\n",
    "       tokens2 = self.tokenizer.tokenize(predicate2)\n",
    "       debug_print(f\"Tokens2: {tokens2}\")\n",
    "       parser2 = Parser(tokens2)\n",
    "       ast2 = parser2.parse()\n",
    "       debug_print(f\"Parsed AST2: {ast2}\")\n",
    "\n",
    "       # Convert ASTs to SymPy expressions\n",
    "       expr1 = self._to_sympy_expr(ast1)\n",
    "       expr2 = self._to_sympy_expr(ast2)\n",
    "\n",
    "       # Simplify expressions\n",
    "       debug_print(f\"SymPy Expression 1: {expr1}\")\n",
    "       simplified_expr1 = sp.simplify(expr1)\n",
    "       debug_print(f\"Simplified SymPy Expression 1: {simplified_expr1}\")\n",
    "\n",
    "       debug_print(f\"SymPy Expression 2: {expr2}\")\n",
    "       simplified_expr2 = sp.simplify(expr2)\n",
    "       debug_print(f\"Simplified SymPy Expression 2: {simplified_expr2}\")\n",
    "\n",
    "       # Manually check implications\n",
    "       implies1_to_2 = self._implies(simplified_expr1, simplified_expr2)\n",
    "       debug_print(f\"> Implies expr1 to expr2: {implies1_to_2}\")\n",
    "       implies2_to_1 = self._implies(simplified_expr2, simplified_expr1)\n",
    "       debug_print(f\"> Implies expr2 to expr1: {implies2_to_1}\")\n",
    "\n",
    "       if implies1_to_2 and not implies2_to_1:\n",
    "           return \"The first predicate is stronger.\"\n",
    "       elif implies2_to_1 and not implies1_to_2:\n",
    "           return \"The second predicate is stronger.\"\n",
    "       elif implies1_to_2 and implies2_to_1:\n",
    "           return \"The predicates are equivalent.\"\n",
    "       else:\n",
    "           return \"The predicates are not equivalent and neither is stronger.\"\n",
    "\n",
    "   def _to_sympy_expr(self, ast):\n",
    "       if not ast.children:\n",
    "           try:\n",
    "               # Try converting to int or float if the value is a numeric string\n",
    "               value = float(ast.value) if '.' in ast.value else int(ast.value)\n",
    "               return sp.Number(value)\n",
    "           except ValueError:\n",
    "               # If conversion fails, treat it as a symbol\n",
    "               return sp.Symbol(ast.value.replace('.', '_'))\n",
    "       args = [self._to_sympy_expr(child) for child in ast.children]\n",
    "       if ast.value in ('&&', '||', '!', '==', '!=', '>', '<', '>=', '<='):\n",
    "           return getattr(sp, self._sympy_operator(ast.value))(*args)\n",
    "       elif ast.value == '/':\n",
    "           return sp.Mul(sp.Pow(args[1], -1), args[0])\n",
    "       elif ast.value == '+':\n",
    "           return sp.Add(*args)\n",
    "       elif ast.value == '-':\n",
    "           return sp.Add(args[0], sp.Mul(-1, args[1]))\n",
    "       elif ast.value == '*':\n",
    "           return sp.Mul(*args)\n",
    "       elif '()' in ast.value:\n",
    "           func_name = ast.value.replace('()', '')\n",
    "           return sp.Function(func_name)(*args)\n",
    "       return sp.Symbol(ast.value.replace('.', '_'))\n",
    "\n",
    "   def _sympy_operator(self, op):\n",
    "       return {\n",
    "           '&&': 'And',\n",
    "           '||': 'Or',\n",
    "           '!': 'Not',\n",
    "           '==': 'Eq',\n",
    "           '!=': 'Ne',\n",
    "           '>': 'Gt',\n",
    "           '<': 'Lt',\n",
    "           '>=': 'Ge',\n",
    "           '<=': 'Le'\n",
    "       }[op]\n",
    "\n",
    "   def _implies(self, expr1, expr2):\n",
    "       \"\"\"\n",
    "       Check if expr1 implies expr2 by manually comparing the expressions.\n",
    "       \"\"\"\n",
    "       debug_print(f\"Checking implication: {expr1} -> {expr2}\")\n",
    "       if expr1 == expr2:\n",
    "           debug_print(\"Expressions are identical.\")\n",
    "           return True\n",
    "\n",
    "       # Handle AND expression for expr2\n",
    "       if isinstance(expr2, And):\n",
    "           # expr1 should imply all parts of expr2 if expr2 is an AND expression\n",
    "           results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "           debug_print(f\"Implication results for And expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "           return all(results)\n",
    "      \n",
    "       # Handle AND expression for expr1\n",
    "       if isinstance(expr1, And):\n",
    "           # All parts of expr1 should imply expr2 if expr1 is an AND expression\n",
    "           results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "           debug_print(f\"Implication results for And expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "           return any(results)\n",
    "\n",
    "       # Handle OR expression for expr2\n",
    "       if isinstance(expr2, Or):\n",
    "           # expr1 should imply at least one part of expr2 if expr2 is an OR expression\n",
    "           results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "           debug_print(f\"Implication results for Or expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "           return any(results)\n",
    "      \n",
    "       # Handle OR expression for expr1\n",
    "       if isinstance(expr1, Or):\n",
    "           # All parts of expr1 should imply expr2 if expr1 is an OR expression\n",
    "           results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "           debug_print(f\"Implication results for Or expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "           return all(results)\n",
    "       \n",
    "       # Handle function calls\n",
    "       if isinstance(expr1, sp.Function) and isinstance(expr2, sp.Function):\n",
    "           # Ensure the function names and the number of arguments match\n",
    "           if expr1.func == expr2.func and len(expr1.args) == len(expr2.args):\n",
    "               return all(self._implies(arg1, arg2) for arg1, arg2 in zip(expr1.args, expr2.args))\n",
    "           return False\n",
    "       \n",
    "       if isinstance(expr1, sp.Symbol) and isinstance(expr2, sp.Symbol):\n",
    "           return expr1 == expr2\n",
    "\n",
    "       # Specific relational operator checks for numerical comparisons\n",
    "       relational_operators = (sp.Gt, sp.Ge, sp.Lt, sp.Le, sp.Eq, sp.Ne)\n",
    "       if isinstance(expr1, relational_operators) and isinstance(expr2, relational_operators):\n",
    "           debug_print(f'we are here!... expr1: {expr1}, expr2: {expr2}')\n",
    "           # Check for Eq vs non-Eq comparisons; we don't handle this well, let's return False\n",
    "           if (isinstance(expr1, sp.Eq) and not isinstance(expr2, sp.Eq)) or (not isinstance(expr1, sp.Eq) and isinstance(expr2, sp.Eq)):\n",
    "               return False  # Handle Eq vs non-Eq cases explicitly\n",
    "           \n",
    "           if all(isinstance(arg, (sp.Float, sp.Integer, sp.Symbol)) for arg in [expr1.lhs, expr1.rhs, expr2.lhs, expr2.rhs]):\n",
    "               debug_print(f'Inside!... expr1: {expr1}, expr2: {expr2}')\n",
    "               # Check if the negation of the implication is not satisfiable\n",
    "               try:\n",
    "                   negation = sp.And(expr1, Not(expr2))\n",
    "                   debug_print(f\"Negation of the implication {expr1} -> {expr2}: {satisfiable(negation)}; type of {type(satisfiable(negation))}\")\n",
    "                   result = not satisfiable(negation, use_lra_theory=True)\n",
    "                   debug_print(f\"Implication {expr1} -> {expr2} using satisfiable: {result}\")\n",
    "                   return result\n",
    "               except Exception as e:\n",
    "                   print(f\"Exception: {e}\")\n",
    "                   return False\n",
    "\n",
    "       #Check if the negation of the implication is not satisfiable for general expressions\n",
    "       debug_print(f'Expression 1 is: {expr1}, and its type is {type(expr1)}')\n",
    "       debug_print(f'Expression 2 is: {expr2}, and its type is {type(expr2)}')\n",
    "       negation = sp.And(expr1, Not(expr2))\n",
    "       result = not satisfiable(negation, use_lra_theory=True) # here.\n",
    "       debug_print(f\"Implication {expr1} -> {expr2} using satisfiable: {result}\")\n",
    "       return result\n",
    "       \n",
    "    #    just return False for all other cases we haven't taken into account  \n",
    "    #    return False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "predicate1 = \"_getIdentifierWhitelist().isIdentifierSupported(_priceIdentifier)\"\n",
    "predicate2 = \"_getIdentifierWhitelist().isIdentifierSupported(smt)\"\n",
    "\n",
    "# predicate1 = \"(_tTotalpercentBuy)/divisorBuy>=(_tTotal/5000)\" \n",
    "# predicate2 = \"(percentBuy_decimals)/divisorBuy>=(_tTotal/10000)\"\n",
    "\n",
    "\n",
    "# predicate1 = \"(_tTotalpercentBuy)/divisorBuy>=(10000/5000)\" \n",
    "# predicate2 = \"(_tTotalpercentBuy)/divisorBuy>=(10000/7000)\"\n",
    "\n",
    "# predicate1 = \"12 < a\"\n",
    "# predicate2 = \"a < 12\"\n",
    "\n",
    "\n",
    "\n",
    "# predicate1 = \"a < 12\"\n",
    "# predicate2 = \"a < 13\"\n",
    "\n",
    "# predicate1 = \"a < 12\"\n",
    "# predicate2 = \"a == 12\"\n",
    "\n",
    "\n",
    "predicate1 = \"balanceOf(to)<=holdLimitAmount-amount\" \n",
    "predicate2 = \"balanceOf(to)+amount<=holdLimitAmount\" \n",
    "\n",
    "comparator = Comparator()\n",
    "result = comparator.compare(predicate1, predicate2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from typing import Union\n",
    "from src.parser import ASTNode\n",
    "from src.config import debug_print\n",
    "\n",
    "class Simplifier:\n",
    "    def __init__(self):\n",
    "        self.symbols = {\n",
    "            'msg.sender': sp.Symbol('msg_sender'),\n",
    "            'msg.origin': sp.Symbol('msg_origin'),\n",
    "            '==': sp.Eq,\n",
    "            '!=': sp.Ne,\n",
    "            '>=': sp.Ge,\n",
    "            '<=': sp.Le,\n",
    "            '>': sp.Gt,\n",
    "            '<': sp.Lt,\n",
    "            '&&': sp.And,\n",
    "            '||': sp.Or,\n",
    "            '!': sp.Not\n",
    "        }\n",
    "\n",
    "    def simplify(self, ast: ASTNode) -> Union[str, ASTNode]:\n",
    "        debug_print(f\"Simplifying AST: {ast}\")\n",
    "        sympy_expr = self._to_sympy(ast)\n",
    "        debug_print(f\"Converted to sympy expression: {sympy_expr}\")\n",
    "        simplified_expr = sp.simplify(sympy_expr)\n",
    "        debug_print(f\"Simplified sympy expression: {simplified_expr}\")\n",
    "        simplified_ast = self._to_ast(simplified_expr)\n",
    "        debug_print(f\"Converted back to AST: {simplified_ast}\")\n",
    "        return simplified_ast\n",
    "\n",
    "    def _to_sympy(self, node: ASTNode):\n",
    "        if node.value in self.symbols and not node.children:\n",
    "            return self.symbols[node.value]\n",
    "        elif node.value in self.symbols:\n",
    "            if node.value in ('&&', '||'):\n",
    "                return self.symbols[node.value](*[self._to_sympy(child) for child in node.children])\n",
    "            elif node.value == '!':\n",
    "                return self.symbols[node.value](self._to_sympy(node.children[0]))\n",
    "            elif len(node.children) == 2:\n",
    "                return self.symbols[node.value](self._to_sympy(node.children[0]), self._to_sympy(node.children[1]))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid number of children for operator {node.value}\")\n",
    "        elif isinstance(node.value, (int, float)):\n",
    "            return sp.Number(node.value)\n",
    "        else:\n",
    "            # Preserve function calls and other identifiers as-is\n",
    "            if '(' in node.value and ')' in node.value:\n",
    "                func_name = node.value  # Ensure the function name is preserved entirely\n",
    "                args = node.children\n",
    "                return sp.Function(func_name)(*map(self._to_sympy, args))\n",
    "            else:\n",
    "                return sp.Symbol(node.value.replace('.', '_'))\n",
    "\n",
    "    def _to_ast(self, expr):\n",
    "        if isinstance(expr, sp.Equality):\n",
    "            return ASTNode('==', [self._to_ast(expr.lhs), self._to_ast(expr.rhs)])\n",
    "        elif isinstance(expr, sp.Rel):\n",
    "            op_map = {'>': '>', '<': '<', '>=': '>=', '<=': '<=', '!=': '!='}\n",
    "            return ASTNode(op_map[expr.rel_op], [self._to_ast(expr.lhs), self._to_ast(expr.rhs)])\n",
    "        elif isinstance(expr, sp.And):\n",
    "            return ASTNode('&&', [self._to_ast(arg) for arg in expr.args])\n",
    "        elif isinstance(expr, sp.Or):\n",
    "            return ASTNode('||', [self._to_ast(arg) for arg in expr.args])\n",
    "        elif isinstance(expr, sp.Not):\n",
    "            return ASTNode('!', [self._to_ast(expr.args[0])])\n",
    "        elif isinstance(expr, sp.Function):\n",
    "            func_name = str(expr.func)\n",
    "            return ASTNode(func_name, [self._to_ast(arg) for arg in expr.args])\n",
    "        else:\n",
    "            return ASTNode(str(expr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "simplifier = Simplifier()\n",
    "\n",
    "tokens1 = tokenizer.tokenize(predicate1)\n",
    "parser1 = Parser(tokens1)\n",
    "ast1 = parser1.parse()\n",
    "simplified_ast1 = simplifier.simplify(ast1)\n",
    "\n",
    "print(f\"predicate1: {predicate1}\")\n",
    "print(f\"AST1 is: {ast1}\")\n",
    "print(f\"Simplified AST1: {simplified_ast1}\")\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------')\n",
    "\n",
    "tokens2 = tokenizer.tokenize(predicate2)\n",
    "parser2 = Parser(tokens2)\n",
    "ast2 = parser2.parse()\n",
    "simplified_ast2 = simplifier.simplify(ast2)\n",
    "\n",
    "print(f\"predicate1: {predicate2}\")\n",
    "print(f\"AST2 is: {ast2}\")\n",
    "print(f\"Simplified AST1: {simplified_ast2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate1, predicate2 = \"(_tTotalpercentBuy)/divisorBuy>=(_tTotal/5000)\", \"(percentBuy_decimals)/divisorBuy>=(_tTotal/10000)\"\n",
    "comparator = Comparator()\n",
    "result = comparator.compare(predicate1, predicate2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"GGmorello/FLAMES_results\", \"100k\", token='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')\n",
    "\n",
    "#ds = load_dataset('GGmorello/FLAMES', 'infilled', split='train[:10000]', token='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', cache_dir='/Users/mojtabaeshghie/.cache/hf')#, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k = ds['train'].to_pandas()\n",
    "head_100 = df_100k.head(100)\n",
    "failures = []\n",
    "\n",
    "for i, row in head_100.iterrows():\n",
    "    pred1 = row['label']\n",
    "    pred2 = row['predicate']\n",
    "    #print(f\"Row {i}: {pred1} vs. {pred2}\")\n",
    "    try:\n",
    "        result = comparator.compare(pred1, pred2)\n",
    "        print(f\"({i}) For predicates {pred1} ************* {pred2} ############## {result}\")\n",
    "    except Exception as e:\n",
    "        failures.append(({'pred1': pred1, 'pred2': pred2, 'exception': e}))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the label and predicates that have `+=` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain \"+=\"\n",
    "filtered_rows = df_100k[(df_100k['label'].str.contains('\\+=', regex=True)) | (df_100k['predicate'].str.contains('\\+=', regex=True))]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing '+=': {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of rows having `days`, `minutes`, and `hours` in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Define the search strings\n",
    "search_strings = ['days', 'minutes', 'hours']\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain any of the search strings\n",
    "filtered_rows = df_100k[\n",
    "    df_100k['label'].str.contains('|'.join(search_strings), regex=True) |\n",
    "    df_100k['predicate'].str.contains('|'.join(search_strings), regex=True)\n",
    "]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing 'days', 'minutes', or 'hours': {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the ones that contain Ethereum currency units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to a pandas DataFrame\n",
    "df_100k = ds['train'].to_pandas()\n",
    "\n",
    "# Define the Ethereum-related search strings\n",
    "ethereum_keywords = ['wei', 'gwei', 'eth']\n",
    "\n",
    "# Filter the rows where 'label' or 'predicate' columns contain any of the Ethereum-related keywords\n",
    "filtered_rows = df_100k[\n",
    "    df_100k['label'].str.contains('|'.join(ethereum_keywords), case=False, regex=True) |\n",
    "    df_100k['predicate'].str.contains('|'.join(ethereum_keywords), case=False, regex=True)\n",
    "]\n",
    "\n",
    "# Add the original indices to the filtered DataFrame\n",
    "filtered_rows = filtered_rows.reset_index(drop=False).rename(columns={'index': 'original_index'})\n",
    "\n",
    "# Display the count of such rows\n",
    "count = len(filtered_rows)\n",
    "print(f\"Number of rows containing Ethereum-related keywords: {count}\")\n",
    "\n",
    "# Display the DataFrame with original index and both 'label' and 'predicate' columns\n",
    "print(filtered_rows[['original_index', 'label', 'predicate']])\n",
    "\n",
    "# If you want to store it for further viewing, you can save it to a new DataFrame\n",
    "filtered_rows_for_viewing = filtered_rows[['original_index', 'label', 'predicate']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling time constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(message):\n",
    "    print(f\"{message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_patterns = [\n",
    "            (r'\\b\\d+\\s*(seconds|minutes|hours|days|weeks)\\b', 'TIME_UNIT'),  # Handle time units first\n",
    "            (r'\\bmsg\\.sender\\b', 'MSG_SENDER'),\n",
    "            (r'\\bmsg\\.origin\\b', 'MSG_ORIGIN'),\n",
    "            (r'\\brequire\\b', 'REQUIRE'),\n",
    "            (r'==', 'EQUAL'),\n",
    "            (r'!=', 'NOT_EQUAL'),\n",
    "            (r'>=', 'GREATER_EQUAL'),\n",
    "            (r'<=', 'LESS_EQUAL'),\n",
    "            (r'>', 'GREATER'),\n",
    "            (r'<', 'LESS'),\n",
    "            (r'&&', 'AND'),\n",
    "            (r'\\|\\|', 'OR'),\n",
    "            (r'\\!', 'NOT'),\n",
    "            (r'&', 'BITWISE_AND'),\n",
    "            (r'\\?', 'QUESTION'),\n",
    "            (r':', 'COLON'),\n",
    "            (r'\\(', 'LPAREN'),\n",
    "            (r'\\)', 'RPAREN'),\n",
    "            (r'\\+', 'PLUS'),\n",
    "            (r'\\-', 'MINUS'),\n",
    "            (r'\\*', 'MULTIPLY'),\n",
    "            (r'\\/', 'DIVIDE'),\n",
    "            (r'\\%', 'MODULUS'),\n",
    "            (r'\\.', 'DOT'),\n",
    "            (r',', 'COMMA'),\n",
    "            (r'=', 'ASSIGN'),\n",
    "            (r'\\[', 'LBRACKET'),\n",
    "            (r'\\]', 'RBRACKET'),\n",
    "            (r'\\\"[^\\\"]*\\\"', 'STRING_LITERAL'),\n",
    "            (r'\\b\\d+\\.\\d+\\b', 'FLOAT'),\n",
    "            (r'\\b\\d+\\b', 'INTEGER'),\n",
    "            (r'\\btrue\\b', 'TRUE'),\n",
    "            (r'\\bfalse\\b', 'FALSE'),\n",
    "            (r'0x[0-9a-fA-F]{40}', 'ADDRESS_LITERAL'),\n",
    "            (r'0x[0-9a-fA-F]+', 'BYTES_LITERAL'),\n",
    "            (r'[a-zA-Z_]\\w*', 'IDENTIFIER'),\n",
    "            (r'\\s+', None),  # Let's ignore whitespace(s)\n",
    "        ]\n",
    "        self.time_units = {\n",
    "            'seconds': 1,\n",
    "            'minutes': 60,\n",
    "            'hours': 3600,\n",
    "            'days': 86400,\n",
    "            'weeks': 604800,\n",
    "        }\n",
    "\n",
    "    def normalize(self, predicate: str) -> str:\n",
    "        predicate = re.sub(r'\\s+', '', predicate)\n",
    "        predicate = re.sub(r'([!=<>]=?)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'(\\&\\&|\\|\\|)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'\\(', r' ( ', predicate)\n",
    "        predicate = re.sub(r'\\)', r' ) ', predicate)\n",
    "        predicate = re.sub(r'\\s+', ' ', predicate)\n",
    "        return predicate.strip()\n",
    "\n",
    "    def tokenize(self, predicate: str) -> List[Tuple[str, str]]:\n",
    "        tokens = []\n",
    "        position = 0\n",
    "        length = len(predicate)\n",
    "\n",
    "        while position < length:\n",
    "            match = None\n",
    "            for pattern, tag in self.token_patterns:\n",
    "                regex = re.compile(pattern)\n",
    "                match = regex.match(predicate, position)\n",
    "                if match:\n",
    "                    if tag:\n",
    "                        value = match.group(0)\n",
    "                        if tag == 'TIME_UNIT':\n",
    "                            number, unit = re.match(r'(\\d+)\\s*(\\w+)', value).groups()\n",
    "                            value = str(int(number) * self.time_units[unit])\n",
    "                            tag = 'INTEGER'\n",
    "                        tokens.append((value, tag))\n",
    "                    position = match.end()\n",
    "                    break\n",
    "            if not match:\n",
    "                if predicate[position] == '(':\n",
    "                    tokens.append(('(', 'LPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ')':\n",
    "                    tokens.append((')', 'RPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ',':\n",
    "                    tokens.append((',', 'COMMA'))\n",
    "                    position += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {predicate[position]} at position {position}\")\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.position = 0\n",
    "\n",
    "    def parse(self) -> ASTNode:\n",
    "        self.position = 0  # Reset the position for each new parse\n",
    "        return self.expression()\n",
    "\n",
    "    def expression(self) -> ASTNode:\n",
    "        node = self.logical_term()\n",
    "        debug_print(f\"Parsed term: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('AND', 'OR'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in expression: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.logical_term()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed expression with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def logical_term(self) -> ASTNode:\n",
    "        node = self.equality()\n",
    "        debug_print(f\"Parsed equality: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('EQUAL', 'NOT_EQUAL'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in logical_term: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.equality()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed logical_term with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def equality(self) -> ASTNode:\n",
    "        node = self.relational()\n",
    "        debug_print(f\"Parsed relational: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('LESS', 'LESS_EQUAL', 'GREATER', 'GREATER_EQUAL'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in equality: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.relational()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed equality with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def relational(self) -> ASTNode:\n",
    "        node = self.term()\n",
    "        debug_print(f\"Parsed term in relational: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('PLUS', 'MINUS'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in relational: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.term()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed relational with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def term(self) -> ASTNode:\n",
    "        node = self.factor()\n",
    "        debug_print(f\"Parsed factor in term: {node}\")\n",
    "\n",
    "        while self.position < len(self.tokens) and self.tokens[self.position][1] in ('MULTIPLY', 'DIVIDE', 'MODULUS'):\n",
    "            operator = self.tokens[self.position]\n",
    "            debug_print(f\"Parsing operator in term: {operator}\")\n",
    "            self.position += 1\n",
    "            right = self.factor()\n",
    "            node = ASTNode(operator[0], [node, right])\n",
    "            debug_print(f\"Parsed term with operator: {node}\")\n",
    "\n",
    "        return node\n",
    "\n",
    "    def factor(self) -> ASTNode:\n",
    "        token = self.tokens[self.position]\n",
    "        debug_print(f\"Parsing factor: {token}\")\n",
    "\n",
    "        if token[1] == 'LPAREN':\n",
    "            self.position += 1\n",
    "            node = self.expression()\n",
    "            self.consume('RPAREN')\n",
    "            return node\n",
    "        elif token[1] in ('IDENTIFIER', 'MSG_SENDER', 'MSG_ORIGIN', 'INTEGER', 'FLOAT'):\n",
    "            # Check if next token is a unit identifier (days, minutes, seconds)\n",
    "            if token[1] == 'INTEGER' and self.position + 1 < len(self.tokens) and self.tokens[self.position + 1][1] == 'IDENTIFIER':\n",
    "                unit_token = self.tokens[self.position + 1]\n",
    "                self.position += 2\n",
    "                combined_value = f\"{token[0]} {unit_token[0]}\"\n",
    "                return ASTNode(combined_value, [])\n",
    "            else:\n",
    "                self.position += 1\n",
    "                return ASTNode(token[0], [])\n",
    "        elif token[1] in ('PLUS', 'MINUS', 'NOT'):\n",
    "            self.position += 1\n",
    "            right = self.factor()\n",
    "            return ASTNode(token[0], [right])\n",
    "\n",
    "        raise ValueError(f\"Unexpected token: {token[1]} at position {self.position}\")\n",
    "\n",
    "    def consume(self, expected_tag):\n",
    "        token = self.tokens[self.position]\n",
    "        debug_print(f\"Consuming token: {token}, expecting: {expected_tag}\")\n",
    "        if token[1] != expected_tag:\n",
    "            raise ValueError(f\"Expected token {expected_tag} but got {token[1]} at position {self.position}\")\n",
    "        self.position += 1\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed AST1: ASTNode(value='<=', children=[ASTNode(value='NS', children=[]), ASTNode(value='604800', children=[])])\n",
      "Parsed AST2: ASTNode(value='<', children=[ASTNode(value='NS', children=[]), ASTNode(value='86400', children=[])])\n",
      "SymPy Expression 1: NS <= 604800\n",
      "Simplified SymPy Expression 1: NS <= 604800\n",
      "SymPy Expression 2: NS < 86400\n",
      "Simplified SymPy Expression 2: NS < 86400\n",
      "we are here!... expr1: NS <= 604800, expr2: NS < 86400\n",
      "Inside!... expr1: NS <= 604800, expr2: NS < 86400\n",
      "Negation of the implication NS <= 604800 -> NS < 86400: {Q.le(NS, 604800): True, Q.ge(NS, 86400): True}; type of <class 'dict'>\n",
      "Implication NS <= 604800 -> NS < 86400 using satisfiable: False\n",
      "> Implies expr1 to expr2: False\n",
      "we are here!... expr1: NS < 86400, expr2: NS <= 604800\n",
      "Inside!... expr1: NS < 86400, expr2: NS <= 604800\n",
      "Negation of the implication NS < 86400 -> NS <= 604800: {Q.lt(NS, 86400): True, Q.gt(NS, 604800): True}; type of <class 'dict'>\n",
      "Implication NS < 86400 -> NS <= 604800 using satisfiable: True\n",
      "> Implies expr2 to expr1: True\n",
      "The second predicate is stronger.\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy.logic.boolalg import And, Or, Not\n",
    "from sympy.logic.inference import satisfiable\n",
    "\n",
    "\n",
    "class Comparator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.simplifier = Simplifier()\n",
    "\n",
    "    def compare(self, predicate1: str, predicate2: str) -> str:\n",
    "        # Tokenize, parse, and simplify the first predicate\n",
    "        tokens1 = self.tokenizer.tokenize(predicate1)\n",
    "        debug_print(f\"Tokens1: {tokens1}\")\n",
    "        parser1 = Parser(tokens1)\n",
    "        ast1 = parser1.parse()\n",
    "        print(f\"Parsed AST1: {ast1}\")\n",
    "\n",
    "        # Tokenize, parse, and simplify the second predicate\n",
    "        tokens2 = self.tokenizer.tokenize(predicate2)\n",
    "        debug_print(f\"Tokens2: {tokens2}\")\n",
    "        parser2 = Parser(tokens2)\n",
    "        ast2 = parser2.parse()\n",
    "        print(f\"Parsed AST2: {ast2}\")\n",
    "\n",
    "        # Convert ASTs to SymPy expressions\n",
    "        expr1 = self._to_sympy_expr(ast1)\n",
    "        expr2 = self._to_sympy_expr(ast2)\n",
    "\n",
    "        # Simplify expressions\n",
    "        print(f\"SymPy Expression 1: {expr1}\")\n",
    "        simplified_expr1 = sp.simplify(expr1)\n",
    "        print(f\"Simplified SymPy Expression 1: {simplified_expr1}\")\n",
    "\n",
    "        print(f\"SymPy Expression 2: {expr2}\")\n",
    "        simplified_expr2 = sp.simplify(expr2)\n",
    "        print(f\"Simplified SymPy Expression 2: {simplified_expr2}\")\n",
    "\n",
    "        # Manually check implications\n",
    "        implies1_to_2 = self._implies(expr1, expr2)\n",
    "        print(f\"> Implies expr1 to expr2: {implies1_to_2}\")\n",
    "        implies2_to_1 = self._implies(expr2, expr1)\n",
    "        print(f\"> Implies expr2 to expr1: {implies2_to_1}\")\n",
    "\n",
    "        if implies1_to_2 and not implies2_to_1:\n",
    "            return \"The first predicate is stronger.\"\n",
    "        elif implies2_to_1 and not implies1_to_2:\n",
    "            return \"The second predicate is stronger.\"\n",
    "        elif implies1_to_2 and implies2_to_1:\n",
    "            return \"The predicates are equivalent.\"\n",
    "        else:\n",
    "            return \"The predicates are not equivalent and neither is stronger.\"\n",
    "\n",
    "    def _to_sympy_expr(self, ast):\n",
    "        if not ast.children:\n",
    "            try:\n",
    "                # Try converting to int or float if the value is a numeric string\n",
    "                value = float(ast.value) if '.' in ast.value else int(ast.value)\n",
    "                return sp.Number(value)\n",
    "            except ValueError:\n",
    "                # If conversion fails, treat it as a symbol\n",
    "                return sp.Symbol(ast.value.replace('.', '_'))\n",
    "        args = [self._to_sympy_expr(child) for child in ast.children]\n",
    "        if ast.value in ('&&', '||', '!', '==', '!=', '>', '<', '>=', '<='):\n",
    "            return getattr(sp, self._sympy_operator(ast.value))(*args)\n",
    "        elif ast.value == '/':\n",
    "            return sp.Mul(sp.Pow(args[1], -1), args[0])\n",
    "        elif ast.value == '+':\n",
    "            return sp.Add(*args)\n",
    "        elif ast.value == '-':\n",
    "            return sp.Add(args[0], sp.Mul(-1, args[1]))\n",
    "        elif ast.value == '*':\n",
    "            return sp.Mul(*args)\n",
    "        elif '()' in ast.value:\n",
    "            func_name = ast.value.replace('()', '')\n",
    "            return sp.Function(func_name)(*args)\n",
    "        return sp.Symbol(ast.value.replace('.', '_'))\n",
    "\n",
    "    def _sympy_operator(self, op):\n",
    "        return {\n",
    "            '&&': 'And',\n",
    "            '||': 'Or',\n",
    "            '!': 'Not',\n",
    "            '==': 'Eq',\n",
    "            '!=': 'Ne',\n",
    "            '>': 'Gt',\n",
    "            '<': 'Lt',\n",
    "            '>=': 'Ge',\n",
    "            '<=': 'Le'\n",
    "        }[op]\n",
    "\n",
    "    def _implies(self, expr1, expr2):\n",
    "        \"\"\"\n",
    "        Check if expr1 implies expr2 by manually comparing the expressions.\n",
    "        \"\"\"\n",
    "        debug_print(f\"Checking implication: {expr1} -> {expr2}\")\n",
    "        if expr1 == expr2:\n",
    "            debug_print(\"Expressions are identical.\")\n",
    "            return True\n",
    "\n",
    "        # Handle AND expression for expr2\n",
    "        if isinstance(expr2, And):\n",
    "            # expr1 should imply all parts of expr2 if expr2 is an AND expression\n",
    "            results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "            debug_print(f\"Implication results for And expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return all(results)\n",
    "      \n",
    "        # Handle AND expression for expr1\n",
    "        if isinstance(expr1, And):\n",
    "            # All parts of expr1 should imply expr2 if expr1 is an AND expression\n",
    "            results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "            debug_print(f\"Implication results for And expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return any(results)\n",
    "\n",
    "        # Handle OR expression for expr2\n",
    "        if isinstance(expr2, Or):\n",
    "            # expr1 should imply at least one part of expr2 if expr2 is an OR expression\n",
    "            results = [self._implies(expr1, arg) for arg in expr2.args]\n",
    "            debug_print(f\"Implication results for Or expr2 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return any(results)\n",
    "      \n",
    "        # Handle OR expression for expr1\n",
    "        if isinstance(expr1, Or):\n",
    "            # All parts of expr1 should imply expr2 if expr1 is an OR expression\n",
    "            results = [self._implies(arg, expr2) for arg in expr1.args]\n",
    "            debug_print(f\"Implication results for Or expr1 which was `{expr1} => {expr2}`: {results}\")\n",
    "            return all(results)\n",
    "       \n",
    "        # Handle function calls\n",
    "        if isinstance(expr1, sp.Function) and isinstance(expr2, sp.Function):\n",
    "            # Ensure the function names and the number of arguments match\n",
    "            if expr1.func == expr2.func and len(expr1.args) == len(expr2.args):\n",
    "                return all(self._implies(arg1, arg2) for arg1, arg2 in zip(expr1.args, expr2.args))\n",
    "            return False\n",
    "       \n",
    "        if isinstance(expr1, sp.Symbol) and isinstance(expr2, sp.Symbol):\n",
    "            return expr1 == expr2\n",
    "\n",
    "        # Specific relational operator checks for numerical comparisons\n",
    "        relational_operators = (sp.Gt, sp.Ge, sp.Lt, sp.Le, sp.Eq, sp.Ne)\n",
    "        if isinstance(expr1, relational_operators) and isinstance(expr2, relational_operators):\n",
    "            print(f'we are here!... expr1: {expr1}, expr2: {expr2}')\n",
    "            # Check for Eq vs non-Eq comparisons; we don't handle this well, let's return False\n",
    "            if (isinstance(expr1, sp.Eq) and not isinstance(expr2, sp.Eq)) or (not isinstance(expr1, sp.Eq) and isinstance(expr2, sp.Eq)):\n",
    "                return False  # Handle Eq vs non-Eq cases explicitly\n",
    "            \n",
    "            if all(isinstance(arg, (sp.Float, sp.Integer, sp.Symbol)) for arg in [expr1.lhs, expr1.rhs, expr2.lhs, expr2.rhs]):\n",
    "                print(f'Inside!... expr1: {expr1}, expr2: {expr2}')\n",
    "                # Check if the negation of the implication is not satisfiable\n",
    "                negation = sp.And(expr1, Not(expr2))\n",
    "                print(f\"Negation of the implication {expr1} -> {expr2}: {satisfiable(negation)}; type of {type(satisfiable(negation))}\")\n",
    "                result = not satisfiable(negation, use_lra_theory=True)\n",
    "                print(f\"Implication {expr1} -> {expr2} using satisfiable: {result}\")\n",
    "                return result\n",
    "       \n",
    "        #    print('We got to the buttom of the function!')\n",
    "        #    # Check if the negation of the implication is not satisfiable for general expressions\n",
    "        #    print(f'Expression 1 is: {expr1}, and its type is {type(expr1)}')\n",
    "        #    print(f'Expression 2 is: {expr2}, and its type is {type(expr2)}')\n",
    "        #    negation = sp.And(expr1, Not(expr2))\n",
    "        #    result = not satisfiable(negation)\n",
    "        #    print(f\"Implication {expr1} -> {expr2} using satisfiable: {result}\")\n",
    "        #    return result\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "predicate1 = \"NS <= (1 weeks)\" \n",
    "predicate2 = \"NS < (1 days)\" \n",
    "\n",
    "comparator = Comparator()\n",
    "result = comparator.compare(predicate1, predicate2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NS', 'IDENTIFIER'), ('<', 'LESS'), ('(', 'LPAREN'), ('86400', 'INTEGER'), (')', 'RPAREN')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_patterns = [\n",
    "            (r'\\b\\d+\\s*(seconds|minutes|hours|days|weeks)\\b', 'TIME_UNIT'),  # Handle time units first\n",
    "            (r'\\bmsg\\.sender\\b', 'MSG_SENDER'),\n",
    "            (r'\\bmsg\\.origin\\b', 'MSG_ORIGIN'),\n",
    "            (r'\\brequire\\b', 'REQUIRE'),\n",
    "            (r'==', 'EQUAL'),\n",
    "            (r'!=', 'NOT_EQUAL'),\n",
    "            (r'>=', 'GREATER_EQUAL'),\n",
    "            (r'<=', 'LESS_EQUAL'),\n",
    "            (r'>', 'GREATER'),\n",
    "            (r'<', 'LESS'),\n",
    "            (r'&&', 'AND'),\n",
    "            (r'\\|\\|', 'OR'),\n",
    "            (r'\\!', 'NOT'),\n",
    "            (r'&', 'BITWISE_AND'),\n",
    "            (r'\\?', 'QUESTION'),\n",
    "            (r':', 'COLON'),\n",
    "            (r'\\(', 'LPAREN'),\n",
    "            (r'\\)', 'RPAREN'),\n",
    "            (r'\\+', 'PLUS'),\n",
    "            (r'\\-', 'MINUS'),\n",
    "            (r'\\*', 'MULTIPLY'),\n",
    "            (r'\\/', 'DIVIDE'),\n",
    "            (r'\\%', 'MODULUS'),\n",
    "            (r'\\.', 'DOT'),\n",
    "            (r',', 'COMMA'),\n",
    "            (r'=', 'ASSIGN'),\n",
    "            (r'\\[', 'LBRACKET'),\n",
    "            (r'\\]', 'RBRACKET'),\n",
    "            (r'\\\"[^\\\"]*\\\"', 'STRING_LITERAL'),\n",
    "            (r'\\b\\d+\\.\\d+\\b', 'FLOAT'),\n",
    "            (r'\\b\\d+\\b', 'INTEGER'),\n",
    "            (r'\\btrue\\b', 'TRUE'),\n",
    "            (r'\\bfalse\\b', 'FALSE'),\n",
    "            (r'0x[0-9a-fA-F]{40}', 'ADDRESS_LITERAL'),\n",
    "            (r'0x[0-9a-fA-F]+', 'BYTES_LITERAL'),\n",
    "            (r'[a-zA-Z_]\\w*', 'IDENTIFIER'),\n",
    "            (r'\\s+', None),  # Let's ignore whitespace(s)\n",
    "        ]\n",
    "        self.time_units = {\n",
    "            'seconds': 1,\n",
    "            'minutes': 60,\n",
    "            'hours': 3600,\n",
    "            'days': 86400,\n",
    "            'weeks': 604800,\n",
    "        }\n",
    "\n",
    "    def normalize(self, predicate: str) -> str:\n",
    "        predicate = re.sub(r'\\s+', '', predicate)\n",
    "        predicate = re.sub(r'([!=<>]=?)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'(\\&\\&|\\|\\|)', r' \\1 ', predicate)\n",
    "        predicate = re.sub(r'\\(', r' ( ', predicate)\n",
    "        predicate = re.sub(r'\\)', r' ) ', predicate)\n",
    "        predicate = re.sub(r'\\s+', ' ', predicate)\n",
    "        return predicate.strip()\n",
    "\n",
    "    def tokenize(self, predicate: str) -> List[Tuple[str, str]]:\n",
    "        tokens = []\n",
    "        position = 0\n",
    "        length = len(predicate)\n",
    "\n",
    "        while position < length:\n",
    "            match = None\n",
    "            for pattern, tag in self.token_patterns:\n",
    "                regex = re.compile(pattern)\n",
    "                match = regex.match(predicate, position)\n",
    "                if match:\n",
    "                    if tag:\n",
    "                        value = match.group(0)\n",
    "                        if tag == 'TIME_UNIT':\n",
    "                            number, unit = re.match(r'(\\d+)\\s*(\\w+)', value).groups()\n",
    "                            value = str(int(number) * self.time_units[unit])\n",
    "                            tag = 'INTEGER'\n",
    "                        tokens.append((value, tag))\n",
    "                    position = match.end()\n",
    "                    break\n",
    "            if not match:\n",
    "                if predicate[position] == '(':\n",
    "                    tokens.append(('(', 'LPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ')':\n",
    "                    tokens.append((')', 'RPAREN'))\n",
    "                    position += 1\n",
    "                elif predicate[position] == ',':\n",
    "                    tokens.append((',', 'COMMA'))\n",
    "                    position += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {predicate[position]} at position {position}\")\n",
    "\n",
    "        return tokens\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(\"NS < (1 days)\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
